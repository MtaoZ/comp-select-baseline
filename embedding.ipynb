{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "# from Dataset.collector import ClassificationCollator\n",
    "# from Dataset import dataset\n",
    "from  Config.config import Config\n",
    "import os \n",
    "# abs_path = os.path.abspath('')\n",
    "# rela_path = '../train.json'\n",
    "# config = Config(config_file=os.path.join(abs_path,rela_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# db = dataset.DatasetBase(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'keyword_id_map',\n",
       " 'keyword_map',\n",
       " 'label_id_map',\n",
       " 'label_map',\n",
       " 'process_sample',\n",
       " 'sample_size',\n",
       " 'token_id_map',\n",
       " 'token_map',\n",
       " 'topic_id_map',\n",
       " 'topic_map',\n",
       " 'word_id_map',\n",
       " 'word_map']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tensor(tensor):\n",
    "    \n",
    "    return torch.nn.init.xavier_normal_(\n",
    "        tensor, gain=torch.nn.init.calculate_gain('linear'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# emb = Embedding(db.label_id_map, 6, config, 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# embedding_lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 dict_map, \n",
    "                 embedding_dim, \n",
    "                 config, \n",
    "                 padding_idx=None,\n",
    "                 pretrained_embedding_file=None\n",
    "                ):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=config.train.dropout)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            len(dict_map), embedding_dim, padding_idx=padding_idx)\n",
    "        embedding_lookup_table = init_tensor(tensor=torch.empty(len(dict_map), embedding_dim))\n",
    "        self.load_pretrained_embedding(embedding_lookup_table, \n",
    "                                       dict_map, \n",
    "                                       embedding_dim, \n",
    "                                       config.train.pretrained_embedding_file\n",
    "                                      )\n",
    "        if padding_idx is not None:\n",
    "            #给这一行赋值为0\n",
    "            embedding_lookup_table[padding_idx] = 0.0\n",
    "        self.embedding.weight.data.copy_(embedding_lookup_table)\n",
    "\n",
    "    def forward(self, vocab_ids, offset=None):\n",
    "\n",
    "        embedding = self.embedding(vocab_ids)\n",
    "\n",
    "        return self.dropout(embedding)\n",
    "\n",
    "    def load_pretrained_embedding(\n",
    "            self, embedding_lookup_table, dict_map, embedding_dim,\n",
    "            pretrained_embedding_file):\n",
    "        if  pretrained_embedding_file == '': return\n",
    "        print(\n",
    "            \"Load embedding from %s\" % (pretrained_embedding_file))\n",
    "        with open(pretrained_embedding_file) as fin:\n",
    "            num_pretrained = 0\n",
    "            for line in fin:\n",
    "                data = line.strip().split(' ')\n",
    "                # Check embedding info\n",
    "#                 if len(data) == 2:\n",
    "#                     assert int(data[1]) == embedding_dim, \\\n",
    "#                         \"Pretrained embedding dim not matching: %s, %d\" % (\n",
    "#                             data[1], embedding_dim)\n",
    "#                     continue\n",
    "                if data[0] not in dict_map:\n",
    "                    continue\n",
    "                embedding = torch.FloatTensor([float(i) for i in data[1:]])\n",
    "                embedding_lookup_table[dict_map[data[0]]] = embedding\n",
    "                num_pretrained += 1\n",
    "        print(\n",
    "            \"Total dict size of is %d\" % (len(dict_map)))\n",
    "        print(\"Size of pretrained embedding is %d\" % (\n",
    "            num_pretrained))\n",
    "        print(\n",
    "            \"Size of randomly initialize embedding is %d\" % (\n",
    "                len(dict_map) - num_pretrained))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import torch.utils.data as Data\n",
    "\n",
    "# # def coll_fn(data_ls):\n",
    "# #     x_ls = []\n",
    "# #     y_ls = []\n",
    "# #     for i in data_ls:\n",
    "# #         x_ls.append(i[0])\n",
    "# #         y_ls.append(i[1])\n",
    "# #     return x_ls,y_ls\n",
    "        \n",
    "    \n",
    "\n",
    "# # class DatasetBase(torch.utils.data.dataset.Dataset):\n",
    "# #     def __init__(self):\n",
    "# #         self.data = [(1,2),(3,4),(4,5)]\n",
    "        \n",
    "# #     def __len__(self):\n",
    "# #         return len(self.data)\n",
    "\n",
    "# #     def __getitem__(self, idx):\n",
    "\n",
    "\n",
    "# #         return self.data[idx]\n",
    "\n",
    "# db = dataset.DatasetBase(config)\n",
    "\n",
    "\n",
    "# batch = 5\n",
    "# #这里是因为label是从1开始计数的\n",
    "# collect = ClassificationCollator(config,len(db.label_map)+1)\n",
    "\n",
    "# loader = Data.DataLoader(\n",
    "#     dataset=db,\n",
    "#     batch_size=batch, # 批大小\n",
    "#     # 若dataset中的样本数不能被batch_size整除的话，最后剩余多少就使用多少\n",
    "#     collate_fn=collect,\n",
    "#     shuffle = True\n",
    "#     )\n",
    "    \n",
    "# for i in loader :\n",
    "#     break\n",
    "# print(i)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
